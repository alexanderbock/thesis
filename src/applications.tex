\chapter{Methodology for Visualization Applications}
\label{cha:visapp}

{\textbf Point of the chapter}
\begin{itemize}
\item "introduces the concepts that i have used in my work"
\item Telling a story that leads up to contribution 
\item Shortest path for a new person to understand the contributions
\item ''This is what people think about collaboration and I did that''
\item ''Supported claims, needs more academic tone''
\item ''Generic, explain the methodolgy''
\end{itemize}

\begin{itemize}
  \item Different kinds of application requirements [Tamara Munzner's papers]. Different stages
  \begin{itemize}
    \item Exploratory for single dataset (information gathering)
    \item Repeating the same processes for multiple datasets (doctors in hospital)
    \item Public dissemination of data
  \end{itemize}
  \item Evaluations
  \begin{itemize}
    \item Many papers in InfoVis on evaluation
  \end{itemize}
  \item Usefulness of convincing a domain expert of the usefulness of a visualization technique
  \begin{itemize}
    \item Obviously, there are visualization techniques that are not intuitively understandable, but useful nontheless
    \item For example: Parallel coordinates plot [USAR evaluation]
  \end{itemize}
\end{itemize}

\subsection*{Collaboration workflow}
The usual workflow of both types of collaborations is the following:
1. Initial contact: Depending on the category of collaboration this is initiated by either the domain expert or the visualization researcher and consists of a cursory introduction into each others fields. The domain scientist provides a limited introduction into the they research topic and the visualization expert brainstorms various techniques that might prove beneficial.
2. Data retrieval: The first step to every collaboration is the development of interfacing techniques. Usually, visualization researchers have access to a growing toolkit of visualization techniques int which the domain experts data has to be imported. In some cases, this might be trivial (for example, loading RAW data), whereas other cases might prove more difficult (for example, converting between grids)
3. Feasibility study: Usually at this step, the visualization researcher has to assess whether there is any potential research in the collaboration. This step is probably one of the most challenging ones as the goals of the different parties might diverge. A novel visualization technique that might be beneficial to the visualization researcher might not be accepted by the domain scientist, complicating the collaboration. This step is furthermore complicated by the fact that some projects do not have an initial payoff in novel visualization techniques, but have the promise of future benefits once an initial collaboration is established. 
4. Application design: In this step, the visualization researcher and the domain expert collaborate in varying degrees on designing the application. On the visualization side, this consists of attaining a cursory knowledge of the subject matter as well as developing the application itself. On the domain expert side it consists of providing knowledge about his field, experimenting with the software, and providing feedback.
5. Publish results: The results of the collaboration are published in both the visualization field as well as the scientific domain of the expert.

\section*{Old}

\begin{itemize}
\item Different phases for applications
\item Different kinds of application requirements
\item Some applications might move between phases
\item Some are never designed to move
\item Interplay between Visualization and applications from a scientific (= domain expert) point of view
\item Important aspect for applications: What are the generalizable aspects that other people can use
\item \cite{munzner2009nested} Main paper; four layer model; validation and iterative loops
\item \cite{tory2004human} Different approaches for system construction (design philosophies)
\item \cite{kirby2013visualization} Sci collaborations across disciplines; interdis, multidis, intradis; definition of a domain expert
\item \cite{van2006bridging} kinds of gaps; knowledge gap and interest gap; different cooperation models
\item \cite{wang2000guidelines} Guidelines for multiview setups in information visualization
\item Combining multiple applications to deal with a problem \cite{rungta2013manyvis}
\item ''Participatory design''
\item Different kinds of application requirements: exploration, production, dissemination
\item Learning about the terminology that experts use
\end{itemize}

\section{Exploration Phase}
\begin{itemize}
\item Exploratory for single dataset (information gathering)
\item Initial information gathering
\item Hypothesis forming (first visualization of a new phenomenon)
\end{itemize}

\section{Production Phase}
\begin{itemize}
\item Repeated information gathering (generating tools for looking at the same kind of data over and over)
\item Applying the same techniques to more datasets
\item Repeating processes on multiple datasets
\end{itemize}

\section{Public Dissemination Phase}
\begin{itemize}
\item Presenting information to the general public
\item Public dissemination
\item Robustness
\item "Publish and perish": Case that even though papers are published, having a public outreach might reach many more people
\item Targeting not domain scientists, but the public audience at large
\item 3D Interaction gestures
\item "In software engineering, validation is about whether one has built the right product, and verification is about whether one has built the product right." (Munzner, Nested Model)


\end{itemize}

\section{Evaluations}
\begin{itemize}
\item \begin{itemize}
    \item Many papers in InfoVis have been written about evaluations
    \item How are they applicable to scivis or vis in general
\end{itemize}
\item \cite{tory2005evaluating} How to perform expert usability studies
\item \cite{kosara2003thoughts} Different reasons for user studies; alternatives to user studies; hard to publish null results
\item \cite{carpendale2008evaluating} InfoVis evaluation; system v system has bias towards familiar system; applicable to scivis? eval has mixture factors; type 1 v type 2 errors; participatory observation (collaborative work with experts)
\item \cite{lewis1993task} Think aloud protocol introduction to the HCI community
\item \cite{ericsson1980verbal} Variation on the think aloud protocol to only mention actions rather than thoughts
\item \cite{likert1932technique} Introduction of the Likert scale
\item \cite{nielsen1994heuristic} Usability heuristics
\item Evaluations \cite{plaisant2004challenge} (how to report evaluations: \cite{forsell2012guide})
\item User studies
\begin{itemize}
  \item Domain expert needs to be involved in finding the peope that want to participate in a study
  \item How do you deal with the requirement of a user study in very limited field? [Space Weather]
\end{itemize}

\end{itemize}

\section{Comparison to Software Engineering}
\begin{itemize}
\item Tamara Munzner's nested design is similar to software engineering waterfall (\cite{royce1970managing, victor2003iterative}) model (make a bigger point out of this?)
\item Make comparisons to Scrum + sprits
\end{itemize}

\section{Domain areas}
\label{app:domain}

\begin{itemize}
\item Moving all of the background information from Chapter 4 into this
\item Define challenges that are picked up by the contributions
\end{itemize}
