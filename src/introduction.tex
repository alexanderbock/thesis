\chapter{Introduction} \label{cha:intro}
This chapter first introduces a general overview of the field of Visualization with its benefits, drawbacks, and dangers in order to provide context for the rest of the thesis.  Second, a commonly used version of the visualization pipeline is introduced including some of its attributes.  The third section presents a short overview of interaction design and its requirements regarding the design of systems where the human-in-the-loop approach is essential.  The last section introduces a classification system for visualization systems that is based on the target audience and usage for which a system is designed.

This chapter is not exhaustive by any means and the inclined reader searching for more detailed information is referred to a number of books for a broader overview of the field.  A general overview of the design of visualization applications is provided by Munzner~\cite{munzner2014visualization}.  Specificially for medical applications, Preim and Botha~\cite{preim2013visual} provide a detailed outline of the use of visual analysis in medicine.

\section{Visualization} \label{cha:intro:vis}
Throughout the years, there have been many attempts at finding a universally accepted definition of the scientific, design, or engineering discipline \emph{Visualization}.  Card~\etal~suggested that Visualization is ``the use of computer-supported, interactive, visual representations of data to amplify cognition''~\cite{card1999readings}.  Other definitions, however, place their focus on the interplay between generated images and the process of subjective visualization in each person~\cite{van2006views}, or generalize the concept of visualization to non-visual phenomana as well~\cite{ronnberg2016interactive}.  While these definitions vary widely, their commonality is the focus on a human observer and the fact that a visualization is created for and by a human, which inherently requires understanding of human physiology and decision making.  Furthermore, the field of visualization was started as, and continues to be, a reaction to the data explosion occurring in other fields~\cite{lorensen2004death}, as a means to make sense of the vast quantities of data that these fields regularly generate and that exceed the human capacity.

Card~\etal~\cite{card1999readings} and many others often separate the field of visualization into at least three categories, \emph{Scientific Visualization}, \emph{Information Visualization}, and \emph{Visual Analytics}.  Scientific Visualization is characterized by the use of data sources with an inherent physical and spatial component.  Data traditionally attributed to Scientific Visualization comes in the form of, for example, simulations or datasets in which the spatial relationship is trivially given.  Information Visualization usually deals with abstract data that does not need to possess an innate spatial component.  Techniques from this part are typically high-dimensional and multi-variate.  Visual Analytics places heavier focus on the analytical reasoning and the interaction modes in order to produce insight into the data rather than the source of the data itself.

\noindent According to Tory~\etal~\cite{tory2002model}, these definitions require the use of words such as ``usually'', ``typically'', or ``traditionally'' which hints at a problem with this type of classification.  For once, it is not always possible to delineate differences between the categories even in the most trivial applications of visualization~\cite{rhyne2003information, weiskopf2006scivis}.  More complicated applications almost always use reseach from two or all three of these categories, increasing the difficulty of a clean classification .  Additionally, from an application domain's point of view, the distinction between different fields might not even be noticable or relevant.  In their work, Tory~\etal~provide a more nuanced \emph{model-based taxonomy} that is focusses on the characteristic of the model of the data rather than the data itself.  Rather than using a taxonomy that is based on the description of the data, they propose a taxonomy that is based on the way the data is used in the visualization system and differentiates between \emph{continous} and \emph{discrete} data, regardless of whether the data itself is abstract or spatial.  In this thesis, there is no distinction between the visualization categories as they are providing different tools to solve the same class of problems, that is, displaying data to a human in order to facilitate insight, and ``at this level there is much more they share than what separates them''~\cite{van2006views}.

\subsection{Benefits} \label{cha:intro:vis:benefits}
As mentioned in the previous chapter, humans are exceptionally well adapted to interpret information contained in images.  This is exemplified by the popular quote that ``a picture is worth a thousand words'', meaning that, for humans, the bandwidth to ingest information visually is much higher than through other representations.  Additionally, the computational complexity of problem classes might differ between the human visual system and computational operations.  This leads to the realization of two classes of problems.  On the one hand, there are problems that can be solved more efficiently by computers, such as searching large databases and algorithms that typically operate on a map-and-reduce scheme.  On the other hand, there are problems solved better by humans, such as pattern recognition, hypothesis forming, and others.  An example of this is detecting proximity among a group of objects, which for the human perception method is of constant complexity ($\mathcal{O}\left( 1 \right)$), and for an algorithm at best linear ($\mathcal{O}\left( n \right)$).  Visualization, being placed on the boundary between these two problem classes, can utilize the respective strengths of both computers and humans through a close integration in order to solve a larger problem set efficiently.

\subsection{Limitations and Dangers} \label{cha:intro:vis:limitations}
One of the important limiting factors influencing each visualization is its subjectiveness.  According to van Wijk, the benefit of using a visualization depends on ```the specification [$\cdots$], the perceptual skills of the observer, and the a priori knowledge of the observer''~\cite{van2005value}.  This realization is yet another reason why close collaboration between the visualization designer and the domain expert is of fundamental importance, as the design process has to take the experts a priori knowledge into account.  Lorensen elaborated on the potential problems for the visualization community that could arise if this collaboration does not occur and summarized it as ``[ Visualization] has lost its customers''~\cite{lorensen2004death}.  The fact that visualization is still alive over a decade later indicates that it indeed was possible for visualization to maintain this collaboration.  Another direct consequence of the subjectiveness is that the reproducibility of a visualization is limited to similar consumers.  A visualization system that is designed for experts in a specific field loses much of its applicability when applied to the same data models from a different field.

Another aspect of the a priori knowledge that is often overlooked is a dependence on cultural background.  Whereas knowledge-based prior information can be assessed empirically, it is much harded to assess cultural bisases.  Some of these cultural differences can be benign, such as the Western tendency to associate movement across a red-green color scale with an increasing value, whereas East Asian cultures would associcate this with a decreasing value, due to the flipped association between the red and green colors.  Other differences can be seen in \fref{fig:intro:vis:lego}, which displays characters from eight cartoon series built from Lego blocks and can be seen as a form of visualization.  Viewed in a culture that is unfamiliar with these cartoon series, however, it becomes easy to see that this visualization will be unable to produce any meaningful results to that group of users.

\begin{figure}
  \centering
  \fbox{\includegraphics[width=\abfboximagewidth]{figures/intro/lego.png}}
  \caption{A collection of advertisement images representing cartoon characters. With the required cultural background, deciphering these visualizations is impossible. Image copyright by Lego.}
  \label{fig:intro:vis:lego}
\end{figure}

% \subsection{Dangers} \label{cha:intro:vis:dangers}
Besides the immense benefits that visualization can provide for supporting data interpretation and hypothesis testing, there are also aspects where the misuse of visualization can have a detrimental effect and pose a danger to the acquisition of insight.  One obvious aspect that is outside the scope of this thesis is the use of visualization to deliberately mislead the audience.  Even without a deliberate attempt, there are many pitfalls that need to be considered when designing a visualization.  Verifying truths, rather than inspiring hypotheses can easily lead to confirmation biases that might lead experts to draw faulty conclusions, exemplified in the quote from van Wijk saying that ``visualization should not be used to verify the final truth, but rather to inspire to new hypotheses, to be checked afterwards''.  Naturally, this danger is most prevalent in the initial exploration stages of a visualization and can be mitigated when a visualization system is matured and applied to many of the same types of datasets; nevertheless, it is an important aspect to consider during the design process.  The remaining dangers fall into one of two categories, \emph{showing incorrect information} and \emph{showing information incorrectly}.  The first category can occur if visualization designers apply wrong assumptions about the data by, for example, applying smoothing to inherently discrete data sets, not handling outliers correctly in a filtering operation, or not considering missing data in real world datasets.  For the domain expert it becomes increasingly difficult to differentiate missing data from outliers, thus eroding the expert's trust in a visualization system.  In the second category, color maps play a huge role.  Using ill-suited color maps it is trivially possible to highlight or hide structures in the data without informing the expert about the process.  One example if this is the continued use of the rainbow color map in science publications even though it has been shown to be inferior to other color maps~\cite{borland2007rainbow}.



\section{Visualization Pipeline} \label{cha:intro:vp}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/intro/pipeline.pdf}
  \caption{One common version of the visualization pipeline as described by Dos Santos and Brodlie~\cite{dos2004gaining}.  The acquired raw data is transformed multiple times until an image is generated that can be used by the user to gain insight.  The ability for the user to control each part of the visualization pipeline is at the heart of the human-in-the-loop methodology.}
  \label{fig:intro:vp}
\end{figure}

\fref{fig:intro:vp} shows a schematic overview of the visualization pipeline.  All basic visualization research aims to improve one or more of these stages in the pipeline and all applied visualization research and systems utilize the concept of this pipeline.  The pipeline used was first described by Haber and McNabb in 1990~\cite{haber1990visualization} and later extended by Dos Santos and Brodlie in 2004~\cite{dos2004gaining}. It consists of four transformations that are successively applied to the incoming data.  For a complete description of the visualization pipeline at its variations, we refer to the two original works or by a survey about the development of the visualization pipeline by Moreland~\cite{moreland2013survey}.

The input to the pipeline is the \emph{Raw data} that is acquired from measurements or simulations.  This data can be structure or unstructured, static or time-varying.  It is then processed by the initial \emph{Data analysis}, which consists of resampling, interpolation, or the removal of outliers.  In the \emph{Filtering} step, the data is reduced with respect to the requirements of the specific task that is to be solved, for example thresholding, level-of-detail selection, or segmentation.  This \emph{Focus data} is then converted in the \emph{Mapping} stage into what Haber and McNabb referred to as \emph{Abstract Visualization Objects}; an abstract object that contains visualization-relatred attributes, such as color, geometry, or texture, that are depend on, but do not necessarily correspond to, the input data.  The final step, \emph{Rendering} uses these abstract representations and generates a final image.  It is only in this last step, where the details of a rendering framework and algorithms, such as OpenGL, are used extensively~\cite{segal2016opengl}.

One important aspect for the design of visualization application systems, that was not fully accounted for in the original visualization pipeline, is the feedback from the user into the various transformation stages.  While it was always possible to change the parameters of the \emph{Rendering} or \emph{Mapping} stages by, for example, changing the camera position, or changing the color attributes of the Abstract Visualization Objects of Geometric Data, the focus of interactivity for the other steps of the pipeline was introduced later.  One of the last feedback loops, Computational Steering, was described by Mulder~\etal~\cite{mulder1999survey}, where the visualization user is able to directly influence the gathering of the \emph{Raw Data} and inspect the results with minimal delay.  Closing this feedback loop leads to the biggest gain in insight as the user can, in the example of simulations, directly understand the influence of parameter changes and can thus gain a deeper understanding of the origin of the data.

\begin{figure}
  \centering
  \fbox{\includegraphics[width=0.5\textwidth]{figures/intro/magicmirror.png}}
  \caption{One example of a multiview visualization technique, a \emph{Magic Mirror}, where X-ray scans of a patients are shown projected to the sides of a combined volumetric rendering.}
  \label{fig:intro:mm}
\end{figure}

An important aspect of the design of visualization systems that is hidden from the pipeline depicted in the figure is the possibility of pipeline branching.  Multiview visualization systems provide multiple simultaneous views on complementary aspects of the data.  One example of these techniques is a magic mirror~\cite{konig1999multiple} as shown in \fref{fig:intro:mm} that presents different aspects of the underlying data projected to the sides of a surrounding cube, thus providing the expert with additional simulataneous information about the data.  In a multiview systems, separate branches of the pipeline handle these different views that are ultimately merged in the \emph{Rendering} step.  There exists a large amount of research on techniques dealing with multiview systems, an example of which is brushing or linking~\cite{tory2003mental}.

\subsection{Data Acquisition} \label{cha:intro:vp:da}
Regardless of its exact composition, the visualization pipeline always has to start with data that is either collected and measured from the real world or generated by simulations.  While in the abstract, the specific form and shape of the data does not influence the visualization pipeline, concrete systems require knowledge about the origin and characteristics of specific data sources.  This section will elaborate on some of the attributes that are important for the contributions that are included in this thesis.

\subsubsection{Data structures} \label{cha:intro:vp:da:datastructures}
There is a variety of methods to structure the data acquired by the aforementioned modalities.  This section introduces a subset of data layouts with a focus on representations that are used in this thesis.  This is by no means a complete reference as each specific problem domain can demand its own optimal data representation.

\paragraph{Point Cloud. }  Sparse point cloud data is the least structured of these data types and consists of, potentially multi-dimensional, measurements in a \nD{2} or \nD{3} space that in general do not possess any connectivity information.  Lidar scanners are a prime example of a modality that generates unstructured point cloud.  Point clouds, due to their unstructured storage, are notoriously difficult to handle and thus pose unique visualization challenges, such as handling transparency, occlusion, and the need for efficient rendering techniques.

\paragraph{Cartesian. }  Multidimensional Cartesian grids are the most widely used form of structured data, with \nD{3} volumetric grids being the most applicable to this thesis.  The uniform structured grid makes it possible to efficiently handle a large amount of data.  A large number of techniques to render this data type exist, for example isosurface rendering or \emph{direct volume rendering}.  The ubiquitious nature of Cartesian volumetric grids also provides a major drawback.  Since it is the de facto standard data format in Scientific Visualization, it is often used in domains that are not space-filling or where an adaptive resolution would be more appropriate, thus resulting in suboptimal storage and access methods where an adaptive grid or a different underlying geometry would be better suited.

\paragraph{Spherical. }  One non-Cartesian grid structure that is used in this work is a grid based on spherical coordinates.  Simulations of the Sun inside our solar system produce higher resolution closer to the origin and automatically possess a spherical symmetry which can be utilized to optimize storage capacity.  In these cases, a spherical data set is a \nD{3} volume, where each of the three spherical coordinate axis, $r$, $\phi$, and $\theta$ is mapped to a Cartesian axis respectively.  When applying direct volume rendering techniques to these datasets, interesting characteristics, such as automatic adaptive sampling or spherical linear interpolation schemes, can be observed~\cite{balabanian2007sonar}.

\subsubsection{Dimensionality} \label{cha:intro:vp:da:dimensionality}
Unfortunately, the word \emph{dimensionality} is overloaded many times in Visualization.  For this section, dimensionality refers to the number of values stored at each location in the dataset, rather than the number of dimensions of the dataset itself.  For the purposes of this thesis a taxonomy, for example as provided by Shneiderman~\cite{shneiderman1996eyes}, is used that describes data as \emph{scalar}, \emph{vector}, \emph{tensor}, or \emph{multi-dimensional}.  The difference between a \nD{3} dataset and a vector dataset is that a vector has additional inherent information that can, and should, be used to restrict the creation of Abstract Visualization Objects.

Each of above categories can also be \emph{time-varying}.  While there have been many techniques that efficiently deal with time-varying datasets, for example time-space partitioning trees~\cite{shen1999fast}, efficient handling of these datasets has not been the focus of this thesis.  As such, this work handles the temporal dimension analogous to the already existing spatial dimensions and, thus, a time-varying datasets as an ordered series of single timestep datasets.

\subsubsection{Data Sources} \label{cha:intro:vp:da:sources}
There are countless potential sources of datasets and a conclusive enumeration them would exceed the scope of any single work.  Instead, this section is presenting a brief overview of the different data modalities and their data acquisition techniques that are being utilized in the contributions section.

\paragraph{X-ray. } X-ray radiation was discovered by Wilhelm R\"ontgen in 1896~\cite{rontgen1896xray} and was quickly developed into a viable imaging technique.  A \emph{source} emits X-ray radiation which passes through the medium that is to be imaged.  Based on the constituent material's absorption coefficients, varying amounts of radiation is absorbed when passing through the object.  A photographic plate on the other side of the object captures the incoming X-ray intensity and thus reconstructs a representation of the object's radiotransparency.  The limitation with this technique is that it is limited to a single \nD{2} projected image of the object in question and can thus not be easily used for a \nD{3} reconstruction.

\paragraph{Computed Tomography. }  A Computed Tomography (CT) scanner works similar to an X-ray detector, in which the source and an electronic detector are rotating around the imaged object.  Throughout this motion, many images are taken by the scanner, which are then reconstructed into a single \nD{3} representation of the object in question.  The Nobel Prize for Physiology or Medicine was awarded to Hounsfield and Cormack in 1979 for their development of this machine~\cite{hounsfield1980computed}.  The spatial and temporal resolution of the scanner and technique has since been improved by multiple orders of magnitude, enabling current machines to perform full-body scans of patients in only a few seconds, or provide the ability to scan a smaller area of interest multiple times per second, thus extending the available information from the structural aspect into the functional domain.  In medical applications the X-ray attenuation is measured in \emph{Hounsfield Units} that measure the attenuation factor of materials and thus provides a standardized scale.  As the X-ray attenuation between different soft tissues is not very high, it is most widely used to study the skeletal structure in humans.

\paragraph{Magnetic Resonance Imaging. }  Magnetic Resonance Imaging (MRI) scanners operate by rapidly manipulating magnetic to force an alignment of the spins of hydrogen atoms and measuring the time it takes an atom to fall back to the ground state.  The radio frequency emitted by this process is detected by the scanner and used to reconstruct a \nD{3} volumetric representation of the object of study.  Since the signal is based off the availablility of hydrogen atoms, MRI scans exhibit the highest resolution in areas with a high water content, such as soft tissue in human patients, whereas the skeletal structure is not accurately represented~\cite{damadian1971tumor}.  Comparing these attributes to a CT scanner shows that a combination of CT/MRI scanners have the capability of providing a high resolution scanner result for a large part of the human body, thus the combination of these two modalities is often used in practice.

\paragraph{Lidar. }  A Lidar scanner is another active scanning device that uses light to a similar effect as radar uses radio waves.  It operates by emitting coherent light and measuring the time until the reflected light returns to the detector, thus making it possible to create a \nD{3} line-of-sight representation of the area surrounding the scanner.  These measurements can be used to create a high-resolution \nD{3} model of, for example, humans or building structures.  Combining a Lidar scanner with other scanning modalities, it becomes possible to not only detect the presence or absense of an obstacle, but also measure other physical attributes, for example surface temperature by measuring radiation emitted by an object or radial velocity through doppler shift.  One important use case for Lidar scanners are autonomous vehicles that can use this information to generate an accurate, real-time \nD{3} local environment that can be used for navigation.

\paragraph{Simulations. }  The previous modalities generate data by measuring physical quantities and thus create a virtual representation of a physical phenomenon which can then be visualized.  Simulations, on the other hand, utilizing a minimal set of physical preconditions and trying to recreate the physical world and thus enabling us insight into areas that would either be infeasible or impossible to investigate directly.  
This enables the recreation of phenomen\ae that all possible scales, but are especially useful in which direct measurements are challenging.  An important distinction between the \emph{image-like} modalities and simulations arises in the form of noise that is introduce into the data.  Whereas simulations have the potential to have a very low signal-to-noise ratio, any image-like modality will always have some form, or multiple forms, of noise attached to the signal that have to be considered in the visual representation.



\subsection{Direct Volume Rendering} \label{cha:intro:vp:dvr}

\begin{wrapfigure}{o}{0.4\textwidth}
\centering
\includegraphics[width=0.39\textwidth]{figures/intro/rendering_integral.pdf}
\caption{Illustration of the rendering integral for a ray with entry point $\vec{x_0}$, exit point $\vec{x_D}$ and an exemplary sampling point $\vec{x}$}
\label{fig:intro:dvr}
\end{wrapfigure}

Most of the work presented in this thesis deals with \nD{3} volumetric data sets for which \emph{direct volume rendering}~(DVR) is a very well-suited and well-established rendering algorithm. It was derived from simplifying general ray tracing algorithms and thus enabling the possibility to be computed at interactive frame rates.  Traditionally, DVR uses a simple emission/absorption model that assumes that the volume is composed of small particles that each have the ability to emit and absorb light, and thus is considered a participating medium~\cite{levoy1988display, drebin1988volume, sabella1988rendering}.  The mathematical formulation of the incoming radiance $I$ was first described by Max~\cite{max1995optical, max2010local}.  Using the definitions in \fref{fig:intro:dvr}, this his descriptions result in:
\begin{equation}
I(\vec{x_c}) = \underbrace{I_0 \left( \vec{x_0} \right) T\left( \vec{x_0}, \vec{x_D} \right)}_{\textrm{Background}} + \int_\vec{x_0}^\vec{x_D} \underbrace{\sigma_\alpha(\vec{x}) I_c(\vec{x})}_{\textrm{Contribution}}  \underbrace{T(\vec{x}, \vec{x_D})}_{\textrm{Attenuation}} \textrm{d} \vec{x},
\label{eqn:intro:dvr}
\end{equation}
\noindent where $I_0$ determines the background illumination, $\sigma_\alpha$ determines whether a sample $\vec{x}$ is emitting or absorbing light and $I_e$ specifies the amount of light contributed at a location $\vec{x}$, attenuated by the attenation factor $T$, given by:
\begin{equation}
T(\vec{a}, \vec{b}) = \exp \left( -\int_\vec{a}^\vec{b} \tau(\vec{x}) \textrm{d} \vec{x} \right),
\label{eqn:intro:dvr2}
\end{equation}
\noindent with $\tau(\vec{x})$ being the extinction coefficient that defines the occusion of light within the volume.  Equations \ref{eqn:intro:dvr} and \ref{eqn:intro:dvr2} combined are known as the volume rendering integral.  For real-world datasets, solving the volume rendering integral analytically is not feasible and are, thus, in practical calculations approximated as Riemann sums with a stepsize $h$ between individual samples.  $h$ is a constant value whose value should be influenced by Nyquist's theorem~\cite{shannon1949communication}.

Many volume rendering techniques can be expressed through modification of Equations~\ref{eqn:intro:dvr} and \ref{eqn:intro:dvr2} or their finite integration step equivalent.  One example is adaptive sampling methods in which the stepsize $h$ depends on the encounted data values, thus being able to provide a higher sampling resolution in different parts of the volume~\cite{danskin1992fast}.  A specialization of this is empty space skipping, where empty parts of the volume are skipped entirely as a performance optimization method~\cite{yagel1993accelerating}.  The volume rendering integral is evaluated for each pixel in the rendering window based off the bounding geometry of the volume that is to be rendered.  By rendering the coordinates of the volume's bounding geometry and storing the results, it is possible to generate each pixel's ray and traverse it in the graphics processing unit (GPU)~\cite{kruger2003acceleration}, which has become the de facto standard in DVR.



\section{Human-in-the-Loop} \label{cha:intro:hitl}
The integration of human perception, cognition, and decision making into the knowledge discovery and analysis process is a vital aspect of any visualization system.  As described by Ward \etal : ``If the goal of visualization is to accurately convey information with pictures, it is essential that perceptual abilities by considered.''~\cite{ward2010interactive}.  For human perception and cognition, examples adhering to the Gestalt theory (as mentioned in Chapter~\ref{cha:motivation}) demonstrate the vast abilities of the human visual system in recognizing clusters, independent from the number of items, based only on simple features such as color, orientation, grouping, or closure.  When designing visualization systems, it is valuable to consider the areas in which human cognition is superior to computational models and vice versa.  The paradigm of creating \emph{Human-in-the-loop} visualization system recognizes that the combination of optimal human cognition and computational models is superior to each separate mechanism~\cite{munzner2014visualization}.  In order to leverage this, the human decision maker needs to be able to influence each individual component of the visualization process.  This influences the visualization pipeline (see \fref{fig:intro:vp}) such that the \emph{Mapping} phase consists of operations that transform data into forms that are more suitable for human consumption and that the human needs to be able to change the parameters of each operation to enable an iterative knowledge gaining process.

In almost all cases the human in-the-loop is an expert in the specific application domain, the \emph{domain expert}, rather than a visualization expert.  The importance of this combination and value of visualization in these aspects has well been recognized~\cite{van2005value}.  This constellation requires the visualization system to be designed in such a way that it is easy and intuitive for the domain expert to understand and control the system and perform the desired tasks.  The design of these visualization system has undergone many studies and potential tasks have been grouped into varying groups~\cite{brehmer2014visualizing}, all of which is elaborated on in the next section.  However, it is important to acknowledge that these designs rarely succeed on the first try and require iteration, thus requiring user studies and repeated design studies, following the overall design principles of software design~\cite{victor2003iterative}.



\section{Visualization Applications} \label{cha:intro:appl}
Visualization applications are one of the large and, arguably, growing fields of research inside the visualization discipline.  As defined by descriptions at the major visualization conferences, ``An application paper normally starts with an encapsulated description of a problem domain and the questions to be resolved by visualization, then describes the application of visualization to the task, any novel techniques developed, and how the visualization solution answered the questions posed. Techniques related to a single problem are normally application papers, and evaluation is often limited because many application papers are essentially custom software for a specific problem.''.

As visualization applications deal with specific needs of a user, these user groups have to be intimately involved with its design and the development.  In many cases these are single applications that combine multiple visualization techniques and, thus, amplifying the contributions of each constituent component.  An example framework for this paradigm is presented by Rungta \etal\ in their ManyVis system~\cite{rungta2013manyvis}.

One widely used technique uses multiple views and linking \& brushing between the views.  The usefulness of multiview setups was shown by North and Shneiderman~\cite{north1997taxonomy}, whereas Wang \etal\ provided guidelines for their usage in visualization~\cite{wang2000guidelines}.  This includes specifying different rules, such as the \emph{Rule of Diversity}, \emph{Rule of Complementarity} and others.



\subsection{Collaborations} \label{cha:intro:appl:collab}
Kirby and Meyer provided an overview about different types of visualization collaborations that can be served by developing an application~\cite{kirby2013visualization}.  In particular with regard to the scientific disciplines that are involved in the project, they highlight three flavors of teams. An \emph{interdisciplinary team} consists of scientists where there is a discipline gap and thus novel problems are solved by combining techniques from multiple distinct disciplines.  \emph{Multidisciplinary} research solves challenges by tightly coupling techniques from distinct disciplines and thus enables solutions that are not solvable by each discipline alone.  Third, \emph{intradisciplinary} research is performed by collaborators from different sides of the same large discipline and fosters the internal cohesion of the scientific discipline.  Placed into the framework put forth by van Wijk, interdisciplinary and intradisciplinary teams would be placed on opposite spectrums of the \emph{knowledge gap} dimension between collaborators~\cite{van2006bridging}.

\begin{figure}
  \centering
  \fbox{\includegraphics[width=\abfboximagewidth]{figures/intro/design_model.pdf}}
  \caption{The nested model of visualization application design introduced by Munzner~\cite{munzner2009nested}\textsuperscript{1}.  Image adopted from Munzner.}
  \label{fig:intro:appl:nested}
\end{figure}


\subsection{Application Design} \label{cha:intro:appl:design}
Tamara Munzner introduced a nested four-layer model that describes the design of visualization applications in 2009~\cite{munzner2009nested}.  Following this model, the design of an application consists of four sequential layers.  An error in validity of a layer impacts the downstream layers, similar to the waterfall modeling in software engineering~\cite{royce1987managing} (see \fref{fig:intro:appl:nested}).  These layers are the \emph{Domain Problem and Data Characterization}, in which the visualization designer immerses themselves in the target domain and vocabulary in order to characterize the workflow of the tasks.  In the \emph{Operation and Data Type Abstraction} layer, this knowledge is converted into a more generic computer science description of the challenges and operations that are required by the desired workflow of the domain expert.  These operations are then converted into visualization components in the \emph{Visual Encoding and Interaction Design} phase in which either novel visualization techniques are designed or previously published techniques are combined to solve the expert's particular problem.  In the last step, the \emph{Algorithm Design} all desired visual encodings are implemented to create the final system, solving potential technical challenges.

Each layer in this nested model has unique threats to its validity that influence the subsequent layers.  For example, a threat to the \emph{Domain Problem} layer would be a misclassification of the domain expert's desired workflow.  Even if the subsequent steps performed successful, the designed application will be unable to fulfill the expert's desires and thus ultimately fail.  However, some of the validation of outer layers can only occur after the downstream layers have already been valided leading to a cascading error if the downstream layers' validation fails.

This nested layers and thread model of application design was later improved by Meyer \etal , which included more fine-grained subdivision within each layer by introducing transactional \emph{blocks} that can be identified in each layer and \emph{guidelines} that describe relationships between blocks.  Using this framework, it becomes possible to characterize the design process of an application system on an abstract level, which makes it possible to analyze and compare different application designs.



\subsection{Application Categories} \label{cha:intro:appl:categories}
One potential categorization of visualization applications is the distinction between \emph{explorational} and \emph{presentational} use cases, which was put forward by, among others, van Wijk~\cite{van2005value}.  It centers on the realization that visualization has to be aimed at different audiences and must adapt accordingly in order to be efficient and effective.  Not taking the intended audience into account, including their prior knowledge and expectations, is a thread to the first layer of Munzner's model and thus invalidates the entire application.

Visualization applications can be categorized by their intended usage and target audience irrepective of their domain. As van Wijk states: ``The main use cases for visualization are exploration (where users do not know what is in the data), and presentation (where some result has to be communicated to others)''~\cite{van2005value}.  A slightly different and in-depth read on this statement leads a placement of visualization applications into three different categories: \emph{Exploration}, \emph{Analysis}, or \emph{Communication}.

An application designed for \emph{Exploration} is targeted towards the initial information gathering and hypothesis generation phase, what van Wijk states as ``where users do not know what is in the data''.  Applications in this category are dominated by a large number of supported features that can be used by the domain expert to dissect their datasets, where the exact result is only vaguely known a priori and unexpected results and discoveries are desired.  An example of this is the work by Ferreira \etal , which provided an application with tools to analyze New York City taxi data in order to find and form hypothesis about urban transportation~\cite{ferreira2013visual}.

The second category of applications, \emph{Analysis}, is also covered by the ``where users do not know what is in the data'' part of van Wijk's characterization.  In this case a prior hypothesis about the data is already known and the application is designed specifically to let the domain expert answer a narrow question about the data.  This category is dominated by repeated usage of the application on different datasets of the same kind.  An example for this is the work by Smit \etal\ in which they presented an application in the medical domain in which they provided tools to enable domain experts to perform repeated multimodal dataset registration~\cite{smit14registrationshop}.

The final category is \emph{Communication} in which a visualization application is used to disseminate tested and confirmed hypotheses to a wide audience.  The audience can be in the same domain as the expert, in which case the application is used in their own publications or grants to communicate their findings, or the general public, in which case the wider public audience is exposed to the confirmed hypothesis for public outreach.  This category also spans applications that are specifically designed for usage by the general public, which in general do not possess the same amount of knowledge about a certain topic as the domain expert or the visualization designer does.  An example of the latter category is the work by J\"onsson \etal , which provided a method to enable museum visitors to design potentially complex transfer functions through the use of an intuitive interface~\cite{jonsson2016intuitive}.
